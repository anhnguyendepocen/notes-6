## Packages for this section

```{r, eval=F}
library(tidyverse)
library(rstan)
```


PASIAS: add a one-parameter and multi-parameter example (look at the schools one in Gelman's book) xxx

## Bayesian and frequentist inference

- The inference philosophy that we have learned so far says that:
  - parameters to be estimated are *fixed* but *unknown*
  - randomness from here: if we were to take another sample, we'd (probably) get different results. 
- This is called "frequentist" or "repeated-sampling" inference.
- Bayesian inference says:
  - *everything* is random, parameters and data, and has some probability distribution
- Ingredients:
  - **prior distribution**: distribution of parameters before seeing data.
  - **likelihood**: distribution of data if the parameters are known (model)
  - **posterior distribution**: distribution of parameters *after* seeing data.
  
## Distribution of parameters

- Instead of having a point or interval estimate of a parameter, we have an entire distribution
- so in Bayesian statistics we can talk about eg.
  - probability that a parameter is bigger than some value
  - probability that a parameter is close to some value
  
- Name comes from Bayes' Theorem, which here says

> posterior is proportional to likelihood times prior

- more discussion about this is in 
[**a blog post**](http://ritsokiguess.site/docs/2018/02/28/working-my-way-back-to-you-a-re-investigation-of-rstan/). 

## An example

- Suppose we have these (integer) observations:

```{r}
(x <- c(0, 4, 3, 6, 3, 3, 2, 4))
```

- Suppose we believe that these come from a Poisson distribution with a mean $\lambda$ that we want to estimate.
- We need a prior distribution for $\lambda$. I will (for some reason) take a $\chi^2$ distribution with 4 df (that has mean 4). Normally this would come from your knowledge of the data-generating *process*.
- The Poisson likelihood can be written down (see over).

## Some algebra

- We have $n=8$ observations $x_i$, so the Poisson likelihood is

$$ \prod_{i=1}^n e^{-\lambda} \lambda^{x_i} = e^{n\lambda} \lambda^S, $$
where $S=\sum_{i=1}^n x_i$. 

- then you write the chi-squared prior density (as a function of $\lambda$):

$$ C \lambda^1 e^{-\lambda/2} $$
where $C$ is a constant

- and then you multiply these together and try to recognize the distributional form. (This one is gamma.)

- Requires too much intelligence (and this is a simple case).

## Sampling from the posterior distribution

- Wouldn't it be nice if we could just *sample* from the posterior distribution? Then we would be able to compute it as accurately as we want.

- Metropolis and Hastings: devise a Markov chain (C62) whose limiting distribution is the posterior you want, and then sample from that Markov chain (easy), allowing enough time to get close enough to the limiting distribution.

- Stan: uses a modern variant that is more efficient (called Hamiltonian Monte Carlo), implemented in R package `rstan`. 

- Write Stan code in a file, compile it and sample from it.

## Components of Stan code: the model

```
model {
// likelihood
x ~ poisson(lambda);
}
```

This is how you say "$X$ has a Poisson distribution with mean $\lambda$".

## Components of Stan code: the prior distribution

```
model {
// prior
lambda ~ chi_square(4);
// likelihood
x ~ poisson(lambda);
}
```

## Components of Stan code: data and parameters (first in the Stan code)

```
data {
int x[8];
}

parameters {
real<lower=0> lambda;
}
```

## Compile and sample from the model

```{r, eval=F}
poisson1_code <- stan_model(file = "poisson1.stan")
```

- set up data

```{r}
poisson1_data <- list(x = x)
```

- sample

```{r, eval=F}
poisson1_fit <- sampling(poisson1_code, data = poisson1_data)
```

```{r, include=F}
poisson1_code=readRDS("poisson1_code.rds")
```

```{r, eval=F}
poisson1_code <- stan_model(file = "poisson1.stan")
```

```{r, echo=F, results="hide", cache=T}
poisson1_data <- list(x = x)
poisson1_fit <- sampling(poisson1_code, data = poisson1_data)
```

```{r, include=F}
saveRDS(poisson1_code, "poisson1_code.rds")
```

## The output

```{r}
poisson1_fit
```

- This summarizes the posterior distribution of $\lambda$; the posterior mean is 3.25, with a 95% posterior interval of 2.15 to 4.66. (The probability that $\lambda$ is between these two values really is 95%.)

## Making the code more general

- The coder in you is probably offended by hard-coding the sample size and the df of the prior distribution. More generally:

```
data {
  int<lower=1> n;
  real<lower=0> prior_df;
  int x[n];
}
...
model {
// prior
lambda ~ chi_square(prior_df);
// likelihood
x ~ poisson(lambda);
}
```

## Set up again and sample:

- Compile again:

```{r, include=F}
poisson2_code=readRDS("poisson2_code.rds")
```


```{r, eval=F}
poisson2_code <- stan_model(file = "poisson2.stan")
```

```{r, include=F}
saveRDS(poisson2_code, "poisson2_code.rds")
```


- set up the data again including the new things we need:

```{r}
poisson2_data <- list(x = x, n = length(x), prior_df = 4)
```

- sample again

```{r, results="hide", cache=T}
poisson2_fit <- sampling(poisson1_code, data = poisson2_data)
```


## output should be the same (to within randomness)

```{r}
poisson2_fit
```

## Extracting actual sampled values

- `rstan` has `extract` for this. There is also an `extract` in `dplyr`: make sure you have the right one.

```{r, fig.height=2.8}
poisson2_out <- extract(poisson2_fit)
ggplot(tibble(lambda = poisson2_out$lambda), aes(x = lambda)) +
  geom_histogram(bins = 20)
```

## Posterior predictive distribution

- Another use for the actual sampled values is to see what kind of *response* values we might get in the future. This should look something like our data. For a Poisson distribution, the response values are integers:

```{r, fig.height=3}
tibble(lambda = poisson2_out$lambda) %>%
  mutate(x_sim = map_int(lambda, ~ rpois(1, .))) -> d
```

## A bar chart:

```{r, fig.height=4}
ggplot(d, aes(x = x_sim)) + geom_bar()
```


## Comparison

Our actual data values were these:

```{r}
x
```

None of these are very unlikely according to our posterior predictive distribution, so our model is believable. 

## A linear regression

xxxx
